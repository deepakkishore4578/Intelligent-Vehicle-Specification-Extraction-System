{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20cd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langchain langchain-community langchain-openai pymupdf faiss-cpu pydantic python-dotenv \n",
    "%pip install langchain-ollama\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14875d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentence-transformers langchain-huggingface \n",
    "!pip install ipywidgets\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078a7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell -2 Imports and API Setup\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# NOTICE: No OpenAI imports here anymore!\n",
    "from langchain_ollama import ChatOllama # New free LLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "# # Securely enter your API Key if not already set in environment\n",
    "# if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = \"sample-service-manual.pdf\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3fb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: sample-service-manual.pdf...\n",
      "--- Preview of Page 24 ---\n",
      "Symptom Chart ‚Äî Suspension System \n",
      "Condition \n",
      "Possible Sources \n",
      "Action \n",
      "z Incorrect thrust \n",
      "angle (dogtracking) \n",
      "z Rear \n",
      "suspension \n",
      "components \n",
      "z INSPECT the rear suspension \n",
      "system. CHECK the rear alignment \n",
      "for the correct thrust angle. \n",
      "REPAIR or INSTALL new \n",
      "suspension components as \n",
      "necessary. REFER to Section 204-\n",
      "02 . \n",
      "z Vehicle drifts/pulls \n",
      "z Unevenly loaded \n",
      "or overloaded \n",
      "vehicle \n",
      "z Tires/tire \n",
      "pressure \n",
      "z Alignment is not \n",
      "within \n",
      "specification \n",
      "z Brake drag \n",
      "z Steering \n",
      "components \n",
      "z GO to Pinpoint Test A . \n",
      "z Front bottoming or \n",
      "riding low \n",
      "z Worn, damaged \n",
      "or incorrect \n",
      "springs \n",
      "z MEASURE the ride height. REFER \n",
      "to Ride Height Measurement in this \n",
      "section. INSTALL new springs as \n",
      "necessary. Refer to the appropriate \n",
      "section in Group 204 for the \n",
      "procedure. \n",
      "z Worn front \n",
      "shock absorbers \n",
      "z INSTALL new shock absorbers as \n",
      "necessary. Refer to the appropriate \n",
      "section in Group 204 for the \n",
      "procedure. \n",
      "z Abnormal/incorrect \n",
      "tire wear \n",
      "z Incorrect tire \n",
      "pressure (rapid \n",
      "cent\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Inspect PDF Text\n",
    "\n",
    "print(f\"Loading PDF: {PDF_PATH}...\")\n",
    "loader = PyMuPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "# Debug: Print the first 500 characters of page 2 \n",
    "# This helps you check if table rows are being read line-by-line or column-by-column.\n",
    "print(f\"--- Preview of Page 24 ---\")\n",
    "print(documents[1].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1b781b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Processing sample-service-manual.pdf with Smart Header Search...\n",
      "‚úÖ Success! Created 1009 smart chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: Smart \"Header Search\" Chunking Strategy  ---\n",
    "import pdfplumber\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def process_pdf_with_header_injection(pdf_path, batch_size=5):\n",
    "    \"\"\"\n",
    "    Reads the PDF. Hunts for the TRUE header row (containing 'Nm' or 'lb-ft') \n",
    "    before processing, ensuring we don't accidentally use page titles as headers.\n",
    "    \"\"\"\n",
    "    print(f\"‚öôÔ∏è Processing {pdf_path} with Smart Header Search...\")\n",
    "    smart_docs = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            # Aggressive table extraction\n",
    "            table = page.extract_table({\n",
    "                \"vertical_strategy\": \"text\", \n",
    "                \"horizontal_strategy\": \"text\"\n",
    "            })\n",
    "            \n",
    "            if table:\n",
    "                # --- LOGIC UPDATE: Find the Real Header ---\n",
    "                headers = None\n",
    "                data_start_idx = 0\n",
    "                \n",
    "                # Look through the first 5 rows to find the true header\n",
    "                for idx, row in enumerate(table[:5]):\n",
    "                    # Check if this row looks like a header (contains units)\n",
    "                    row_str = \" \".join([str(c).lower() for c in row if c])\n",
    "                    if \"nm\" in row_str or \"lb-ft\" in row_str or \"description\" in row_str:\n",
    "                        headers = row\n",
    "                        data_start_idx = idx + 1\n",
    "                        break\n",
    "                \n",
    "                # If we found a valid header, proceed\n",
    "                if headers:\n",
    "                    # Clean headers\n",
    "                    clean_headers = [str(h).replace('\\n', ' ') if h else f\"Col_{j}\" for j, h in enumerate(headers)]\n",
    "                    data_rows = table[data_start_idx:]\n",
    "                    \n",
    "                    current_batch = []\n",
    "                    for row_idx, row in enumerate(data_rows):\n",
    "                        clean_row = [str(cell).replace('\\n', ' ') if cell else \"N/A\" for cell in row]\n",
    "                        \n",
    "                        # Match header length\n",
    "                        if len(clean_headers) == len(clean_row):\n",
    "                            # Create context string: \"Component: Bolt, Nm: 17...\"\n",
    "                            row_context = \", \".join([f\"{h}: {r}\" for h, r in zip(clean_headers, clean_row)])\n",
    "                            current_batch.append(row_context)\n",
    "                        \n",
    "                        if len(current_batch) >= batch_size or row_idx == len(data_rows) - 1:\n",
    "                            if current_batch:\n",
    "                                doc = Document(\n",
    "                                    page_content=\"\\n\".join(current_batch),\n",
    "                                    metadata={\"source\": pdf_path, \"page\": i + 1, \"type\": \"table_chunk\"}\n",
    "                                )\n",
    "                                smart_docs.append(doc)\n",
    "                                current_batch = []\n",
    "                else:\n",
    "                    # If no unit header found, fallback to text (safer than using a bad header)\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        smart_docs.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i+1}))\n",
    "            else:\n",
    "                # Fallback if no table detected\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    smart_docs.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i+1}))\n",
    "\n",
    "    return smart_docs\n",
    "\n",
    "# --- EXECUTE & RELOAD ---\n",
    "pdf_filename = \"sample-service-manual.pdf\" \n",
    "try:\n",
    "    chunks = process_pdf_with_header_injection(pdf_filename)\n",
    "    print(f\"‚úÖ Success! Created {len(chunks)} smart chunks.\")\n",
    "    \n",
    "    # RELOAD DATABASE\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    # Re-run embedding (Cell 5 logic)\n",
    "    # vector_store = FAISS.from_documents(chunks, embeddings) \n",
    "    # vector_store.save_local(\"faiss_db_index\")\n",
    "    # print(\"‚úÖ Database updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23011a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local embedding model ...\n",
      "Creating vector store...\n",
      "Vector store created successfully using Sentence Transformers!\n"
     ]
    }
   ],
   "source": [
    "# ---CELL 5: Vector Store with Sentence Transformers ---\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS \n",
    "\n",
    "print(\"Loading local embedding model ...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store created successfully using Sentence Transformers!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644d3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top Retrieval Result for 'Torque specifications for suspension' ---\n",
      "2014 F-150 Workshop Manual Page 1sur 1\n",
      "SECTION 204-01A: Front Suspension ‚ÄîRear Wheel Drive (RWD) 2014 F-150 Workshop Manual\n",
      "SPECIFICATIONS Procedure revision date: 10/25/2013\n",
      "Torque Specifications\n",
      "Description Nm lb-ft lb-in\n",
      "Brake disc shield bolts 17 ‚Äî 150\n",
      "Brake hose bracket bolt 12 ‚Äî 106\n",
      "Lower arm forward and rearward nuts 350 258 ‚Äî\n",
      "Lower ball joint nut 175 129 ‚Äî\n",
      "Shock absorber lower nuts 90 66 ‚Äî\n",
      "Shock absorber upper mount nuts 63 46 ‚Äî\n",
      "Shock rod nut 55 41 ‚Äî\n",
      "Stabilizer bar bracket nuts 55 41 ‚Äî\n",
      "Stabilizer bar link nuts 70 52 ‚Äî\n",
      "Tie-rod end nut 115 85 ‚Äî\n",
      "Upper arm-to-frame nuts 150 111 ‚Äî\n",
      "Upper ball joint nut 115 85 ‚Äî\n",
      "Wheel bearing and wheel hub bolts 175 129 ‚Äî\n",
      "Wheel speed sensor bolt 18 ‚Äî 159\n",
      "Wheel speed sensor harness bracket bolt 12 ‚Äî 106\n",
      "file:///C:/TSO/tsocache/VDTOM2_10764/SE2~us~en~file=SE241A01.HTM~gen~ref.H... 2014-03-01\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test Retrieval (Debugging Step)\n",
    "test_query = \"Torque specifications for suspension\"\n",
    "results = vector_store.similarity_search(test_query, k=10)\n",
    "\n",
    "print(f\"--- Top Retrieval Result for '{test_query}' ---\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba926746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Define Output Structure\n",
    "\n",
    "class VehicleSpec(BaseModel):\n",
    "    \"\"\"Information about a specific vehicle specification.\"\"\"\n",
    "    component: str = Field(..., description=\"The specific part or component name (e.g., 'Brake Caliper Bolt').\")\n",
    "    spec_type: str = Field(..., description=\"The type of specification (e.g., 'Torque', 'Capacity', 'Clearance').\")\n",
    "    value: str = Field(..., description=\"The numerical value of the specification.\")\n",
    "    unit: Optional[str] = Field(None, description=\"The unit of measurement (e.g., 'Nm', 'lb-ft', 'L').\")\n",
    "\n",
    "class SpecList(BaseModel):\n",
    "    \"\"\"A list of extracted vehicle specifications.\"\"\"\n",
    "    specs: List[VehicleSpec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdbad88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Batch Extraction Job...\n",
      "   Processing: Torque specifications for front suspension...\n",
      "   ‚úÖ Found 40 items in 3.28s.\n",
      "   ‚è≥ Sleeping 10s to respect API limits...\n",
      "   Processing: Torque for lower ball joint...\n",
      "   ‚úÖ Found 2 items in 0.98s.\n",
      "   ‚è≥ Sleeping 10s to respect API limits...\n",
      "   Processing: Torque specifications for braking system...\n",
      "   ‚úÖ Found 141 items in 9.88s.\n",
      "   ‚è≥ Sleeping 10s to respect API limits...\n",
      "   Processing: Fluid capacities...\n",
      "   ‚úÖ Found 6 items in 3.18s.\n",
      "   ‚è≥ Sleeping 10s to respect API limits...\n",
      "\n",
      "üéâ DONE! Saved 189 total specs.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 8: Main Extraction Loop (Optimized for Smart Chunks) ---\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP CLOUD LLM\n",
    "# ==========================================\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: Bulletproof JSON Extractor\n",
    "# ==========================================\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match: return json.loads(match.group(0))\n",
    "        return json.loads(text)\n",
    "    except: return None\n",
    "\n",
    "# ==========================================\n",
    "# 3. MASTER PROMPT (Optimized for Header Injection)\n",
    "# ==========================================\n",
    "prompt_template = \"\"\"\n",
    "You are a highly accurate technical data extractor.\n",
    "Analyze the provided text context and extract specifications for: '{question}'.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. **TRUST EXPLICIT LABELS**: The text often contains explicit keys like \"Nm: 175\" or \"lb-ft: 129\". Use these labels to identify values and units.\n",
    "2. **FALLBACK PATTERN**: If labels are missing or look like \"Column_1\", use the standard manual pattern: **Nm** (1st number) -> **lb-ft** (2nd) -> **lb-in** (3rd).\n",
    "3. **EXTRACT EVERYTHING**: If the context lists multiple components (e.g., \"Nut\", \"Bolt\", \"Link\"), extract ALL of them.\n",
    "\n",
    "Output JSON: {{ \"specs\": [ {{ \"component\": \"...\", \"spec_type\": \"...\", \"value\": \"...\", \"unit\": \"...\" }} ] }}\n",
    "If no relevant data is found, return: {{ \"specs\": [] }}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "queries = [\n",
    "    \"Torque specifications for front suspension\",\n",
    "    \"Torque for lower ball joint\",  # Added your specific failing query\n",
    "    \"Torque specifications for braking system\",\n",
    "    \"Fluid capacities\"\n",
    "]\n",
    "\n",
    "all_extracted_data = []\n",
    "\n",
    "print(\"üöÄ Starting Batch Extraction Job...\")\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"   Processing: {query}...\")\n",
    "    start_ts = time.time()\n",
    "    \n",
    "    # --- FIX IS HERE: Increased k from 3 to 6 ---\n",
    "    # Since we broke tables into small 5-row chunks, we need to retrieve MORE chunks\n",
    "    # to ensure we capture the specific row the user asked for.\n",
    "    docs = vector_store.similarity_search(query, k=12) \n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Generate\n",
    "    try:\n",
    "        chain = ChatPromptTemplate.from_template(prompt_template) | llm\n",
    "        response = chain.invoke({\"context\": context, \"question\": query})\n",
    "        \n",
    "        # Parse\n",
    "        data = extract_json_from_text(response.content)\n",
    "        \n",
    "        if data and \"specs\" in data:\n",
    "            items = data[\"specs\"]\n",
    "            # Filter out empty results\n",
    "            if items:\n",
    "                all_extracted_data.extend(items)\n",
    "                print(f\"   ‚úÖ Found {len(items)} items in {time.time()-start_ts:.2f}s.\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Valid JSON, but no specific data found.\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No JSON found.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "    # ---  PAUSE FOR 10 SECONDS ---\n",
    "    print(\"   ‚è≥ Sleeping 10s to respect API limits...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "# Save\n",
    "with open(\"vehicle_specs.json\", \"w\") as f:\n",
    "    json.dump(all_extracted_data, f, indent=4)\n",
    "\n",
    "print(f\"\\nüéâ DONE! Saved {len(all_extracted_data)} total specs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2969ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to vehicle_specs.json\n",
      "[\n",
      "  {\n",
      "    \"component\": \"Brake disc shield bolts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"17\",\n",
      "    \"unit\": \"Nm\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake disc shield bolts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"150\",\n",
      "    \"unit\": \"lb-in\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake hose bracket bolt\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"12\",\n",
      "    \"unit\": \"Nm\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Brake hose bracket bolt\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"106\",\n",
      "    \"unit\": \"lb-in\"\n",
      "  },\n",
      "  {\n",
      "    \"component\": \"Lower arm forward and rearward nuts\",\n",
      "    \"spec_type\": \"Torque\",\n",
      "    \"value\": \"350\",\n",
      "    \"unit\": \"Nm\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save and View Results\n",
    "\n",
    "import json\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"vehicle_specs.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_extracted_data, f, indent=4)\n",
    "\n",
    "print(f\"Saved data to {output_file}\")\n",
    "\n",
    "# Display first 5 results\n",
    "print(json.dumps(all_extracted_data[:5], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d190478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index saved to folder 'faiss_db_index_test'\n"
     ]
    }
   ],
   "source": [
    "# Run this in your notebook to save the index to disk\n",
    "vector_store.save_local(\"faiss_db_index_test\")\n",
    "print(\"‚úÖ Index saved to folder 'faiss_db_index_test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a90876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15f22e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f918753be3d488b9db2f4bdabfd7f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n<style>\\n    .mechanic-header { \\n        background: #2C3E50; \\n        color: w‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Basic UI -  Mechanic AI BOT(Groq Llama 3.3) ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# ============================================\n",
    "# 1. SETUP GROQ API\n",
    "# ============================================   \n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "if GROQ_API_KEY.startswith(\"PASTE\"):\n",
    "    print(\"‚ö†Ô∏è PLEASE PASTE YOUR ACTUAL GROQ API KEY ABOVE!\")\n",
    "\n",
    "gui_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 2. UNIVERSAL PROMPT\n",
    "# ============================================\n",
    "prompt_template = \"\"\"\n",
    "You are a technical assistant. Extract ALL specifications for: '{question}'.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. **Torque Tables**: Pattern is **Nm** -> **lb-ft** -> **lb-in**.\n",
    "2. **Fluids/Parts**: If no unit exists (like a part number), return null for unit.\n",
    "3. **General**: Extract EVERY row. Clean up values (numbers only for torque).\n",
    "\n",
    "Output JSON: {{ \"specs\": [ {{ \"component\": \"...\", \"spec_type\": \"...\", \"value\": \"...\", \"unit\": \"...\" }} ] }}\n",
    "If empty, return {{ \"specs\": [] }}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# ============================================\n",
    "# 3. HELPER: Bulletproof JSON Cleaner\n",
    "# ============================================\n",
    "def clean_and_parse_json(raw_response):\n",
    "    try:\n",
    "        if isinstance(raw_response, list): text = \"\".join([str(item) for item in raw_response])\n",
    "        else: text = str(raw_response)\n",
    "        \n",
    "        # Remove markdown ticks and find the JSON object\n",
    "        text = text.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match: return json.loads(match.group(0))\n",
    "        return {\"specs\": []}\n",
    "    except:\n",
    "        return {\"specs\": []}\n",
    "\n",
    "# ============================================\n",
    "# 4. PROFESSIONAL UI (Updated CSS for Left Align)\n",
    "# ============================================\n",
    "header_html = \"\"\"\n",
    "<style>\n",
    "    .mechanic-header { \n",
    "        background: #2C3E50; \n",
    "        color: white; \n",
    "        padding: 15px 20px; \n",
    "        border-radius: 8px 8px 0 0; \n",
    "        text-align: left; \n",
    "        font-family: sans-serif; \n",
    "    }\n",
    "    .mechanic-subheader { font-size: 12px; opacity: 0.8; margin-top: 5px; }\n",
    "    \n",
    "    .result-table { \n",
    "        width: 100%; \n",
    "        border-collapse: collapse; \n",
    "        margin-top: 15px; \n",
    "        font-family: sans-serif; \n",
    "        font-size: 14px; \n",
    "        table-layout: fixed; \n",
    "    }\n",
    "    \n",
    "    .result-table th { \n",
    "        background: #34495E; \n",
    "        color: white; \n",
    "        padding: 10px; \n",
    "        text-align: left; /* Header Left Align */\n",
    "    }\n",
    "    \n",
    "    .result-table td { \n",
    "        border-bottom: 1px solid #ddd; \n",
    "        padding: 8px; \n",
    "        color: #333; \n",
    "        word-wrap: break-word;\n",
    "        text-align: left !important; /* <--- FORCE LEFT ALIGNMENT ON ROWS */\n",
    "    }\n",
    "    \n",
    "    .result-table tr {\n",
    "        text-align: left !important;\n",
    "    }\n",
    "    \n",
    "    .result-table th:nth-child(1) { width: 45%; } \n",
    "    .highlight-val { color: #C0392B; font-weight: bold; }\n",
    "</style>\n",
    "<div class=\"mechanic-header\">\n",
    "    <h2>üöó Mechanic AI: Service Hub (Cloud Edition)</h2>\n",
    "    <div class=\"mechanic-subheader\">Powered by Groq Llama 3.3</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "header_widget = widgets.HTML(value=header_html)\n",
    "\n",
    "PRESET_QUERIES = [\n",
    "    \"--- Select a Quick Query ---\",\n",
    "    \"Torque specifications for front suspension\",\n",
    "    \"Torque specifications for braking system\",\n",
    "    \"Service materials\",             \n",
    "    \"Wheel alignment specifications\",\n",
    "    \"Grease and lubricants\"\n",
    "]\n",
    "\n",
    "dropdown = widgets.Dropdown(options=PRESET_QUERIES, layout=widgets.Layout(width='98%'))\n",
    "query_input = widgets.Text(placeholder='...or type a specific question here (PLEASE DO NOT PASTE QUERY)', layout=widgets.Layout(width='98%'))\n",
    "search_btn = widgets.Button(description=' Extract Data', icon='search', button_style='primary', layout=widgets.Layout(width='200px', height='40px'))\n",
    "output_area = widgets.Output(layout={'border': '1px solid #ddd', 'padding': '15px', 'margin_top':'15px', 'min_height': '100px'})\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    if change['new'] != \"--- Select a Quick Query ---\":\n",
    "        query_input.value = change['new']\n",
    "\n",
    "def on_search_click(b):\n",
    "    output_area.clear_output()\n",
    "    q = query_input.value\n",
    "    \n",
    "    if not q or q == \"--- Select a Quick Query ---\":\n",
    "        with output_area: display(HTML(\"<b style='color:orange;'>‚ö†Ô∏è Please select or type a query.</b>\"))\n",
    "        return\n",
    "    \n",
    "    with output_area:\n",
    "        display(HTML(\"<b>‚öôÔ∏è Querying Groq (Llama 3.3)... please wait...</b>\"))\n",
    "        start_ts = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. RETRIEVE\n",
    "            docs = vector_store.similarity_search(q, k=3)\n",
    "            context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "            \n",
    "            # 2. GENERATE\n",
    "            prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "            chain = prompt | gui_llm\n",
    "            response = chain.invoke({\"context\": context, \"question\": q})\n",
    "            \n",
    "            # 3. PARSE\n",
    "            data = clean_and_parse_json(response.content)\n",
    "            specs = data.get(\"specs\", [])\n",
    "            elapsed = time.time() - start_ts\n",
    "            output_area.clear_output()\n",
    "            \n",
    "            if specs:\n",
    "                print(f\"‚úÖ Found {len(specs)} specs in {elapsed:.2f}s:\")\n",
    "                rows = \"\"\n",
    "                for s in specs:\n",
    "                    rows += f\"<tr><td>{s.get('component','-')}</td><td>{s.get('spec_type','-')}</td><td class='highlight-val'>{s.get('value','-')}</td><td>{s.get('unit','') or ''}</td></tr>\"\n",
    "                table = f\"<table class='result-table'><thead><tr><th>Component</th><th>Type</th><th>Value</th><th>Unit</th></tr></thead><tbody>{rows}</tbody></table>\"\n",
    "                display(HTML(table))\n",
    "            else:\n",
    "                display(HTML(f\"<div style='color:red; padding:10px;'>‚ö†Ô∏è No data found for: '{q}'</div>\"))\n",
    "                \n",
    "        except Exception as e:\n",
    "            output_area.clear_output()\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "dropdown.observe(on_dropdown_change, names='value')\n",
    "search_btn.on_click(on_search_click)\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.Label(value=\"Quick Select:\"), dropdown, \n",
    "    widgets.Label(value=\"Custom Query:\"), query_input, \n",
    "    widgets.Box([search_btn], layout=widgets.Layout(margin='15px 0 0 0'))\n",
    "], layout=widgets.Layout(padding='20px'))\n",
    "\n",
    "display(widgets.VBox([header_widget, controls, output_area]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
